{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Disease Classification Model\n",
    "\n",
    "Applies transfer learning on pre-trained ImageNet models to identify if an image of a leaf is either healthy or diseased.\n",
    "\n",
    "Training metrics are logged to TensorBoard for real-time monitoring.\n",
    "\n",
    "`train_path` and `test_path` points to the directory containing the dataset. Each dataset contains two sub-directories for images of each class `healthy` and `diseased`.\n",
    "\n",
    "```\n",
    "train\n",
    "|-- diseased\n",
    "|   |-- diseased-0.jpg\n",
    "|   |-- dieased-1.jpg\n",
    "|   ...\n",
    "|-- healthy\n",
    "|   |-- healthy-0.jpg\n",
    "|   |-- healthy-1.jpg\n",
    "|   ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from utils import train, test, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"dataset/train\"\n",
    "test_path = \"dataset/test\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "\n",
    "# Create output folder\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# Tensorboard writer\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model\n",
    "\n",
    "Run one of the following code blocks to select the pre-trained ImageNet model for transfer learning. Model parameters are frozen besides for the final linear layer. In the case of \"Double FC Output\", the two last linear layers are unfrozen.\n",
    "\n",
    "### A. AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(4096, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet18\"\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. MobileNetv3-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"mobilenetv3-s\"\n",
    "model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[3] = nn.Linear(1024, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. EfficientNet-b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"efficientnet-b4\"\n",
    "model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[1] = nn.Linear(1792, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet50\"\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(2048, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. MobileNetv3-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"mobilenetv3-l\"\n",
    "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[3] = nn.Linear(1280, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. MobileNetv3-L Double FC Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"mobilenetv3-l-doublefc\"\n",
    "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[0] = nn.Linear(960, 1280)\n",
    "model.classifier[3] = nn.Linear(1280, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. MobileNetv3-S Double FC Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"mobilenetv3-s-doublefc\"\n",
    "model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[0] = nn.Linear(576, 1024)\n",
    "model.classifier[3] = nn.Linear(1024, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Transformation\n",
    "Run one of the following code blocks to select the type of image augmentation to apply\n",
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # needed for ImageNet\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AugMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Shuffled Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.AugMix(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # needed for ImageNet\n",
    "])\n",
    "\n",
    "model_name += '-augmix'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Fine tunes the chosen pre-trained model. Trained models are checkpointed every 5 epochs to `./models`. Training metrics are logged to TensorBoard.\n",
    "\n",
    "Models are trainsed using:\n",
    "- Binary Cross Entropy Loss with Logits\n",
    "- Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.695182  [   64/40110]\n",
      "loss: 0.159976  [ 6464/40110]\n",
      "loss: 0.143280  [12864/40110]\n",
      "loss: 0.137013  [19264/40110]\n",
      "loss: 0.125520  [25664/40110]\n",
      "loss: 0.192839  [32064/40110]\n",
      "loss: 0.156215  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.148972 \n",
      " Precision: 0.902894, Recall: 0.959801, f1: 0.930479 \n",
      "\n",
      "Saving Checkpoint epoch: 0\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.138976  [   64/40110]\n",
      "loss: 0.049131  [ 6464/40110]\n",
      "loss: 0.182212  [12864/40110]\n",
      "loss: 0.158138  [19264/40110]\n",
      "loss: 0.078525  [25664/40110]\n",
      "loss: 0.123842  [32064/40110]\n",
      "loss: 0.174247  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.106485 \n",
      " Precision: 0.971049, Recall: 0.923821, f1: 0.946846 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.145739  [   64/40110]\n",
      "loss: 0.233379  [ 6464/40110]\n",
      "loss: 0.147314  [12864/40110]\n",
      "loss: 0.044266  [19264/40110]\n",
      "loss: 0.163156  [25664/40110]\n",
      "loss: 0.154983  [32064/40110]\n",
      "loss: 0.124337  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.089816 \n",
      " Precision: 0.938853, Recall: 0.979156, f1: 0.958581 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.057286  [   64/40110]\n",
      "loss: 0.041618  [ 6464/40110]\n",
      "loss: 0.067728  [12864/40110]\n",
      "loss: 0.071591  [19264/40110]\n",
      "loss: 0.125609  [25664/40110]\n",
      "loss: 0.074993  [32064/40110]\n",
      "loss: 0.107131  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.090470 \n",
      " Precision: 0.937086, Recall: 0.983127, f1: 0.959554 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.104045  [   64/40110]\n",
      "loss: 0.037388  [ 6464/40110]\n",
      "loss: 0.109608  [12864/40110]\n",
      "loss: 0.151148  [19264/40110]\n",
      "loss: 0.101175  [25664/40110]\n",
      "loss: 0.035568  [32064/40110]\n",
      "loss: 0.089494  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.070142 \n",
      " Precision: 0.977924, Recall: 0.956328, f1: 0.967005 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.066026  [   64/40110]\n",
      "loss: 0.091796  [ 6464/40110]\n",
      "loss: 0.105644  [12864/40110]\n",
      "loss: 0.106328  [19264/40110]\n",
      "loss: 0.018383  [25664/40110]\n",
      "loss: 0.067105  [32064/40110]\n",
      "loss: 0.084761  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.067824 \n",
      " Precision: 0.961360, Recall: 0.975434, f1: 0.968346 \n",
      "\n",
      "Saving Checkpoint epoch: 5\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.101394  [   64/40110]\n",
      "loss: 0.034335  [ 6464/40110]\n",
      "loss: 0.027620  [12864/40110]\n",
      "loss: 0.147319  [19264/40110]\n",
      "loss: 0.069969  [25664/40110]\n",
      "loss: 0.166677  [32064/40110]\n",
      "loss: 0.193711  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.061182 \n",
      " Precision: 0.967655, Recall: 0.979901, f1: 0.973739 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.017940  [   64/40110]\n",
      "loss: 0.021992  [ 6464/40110]\n",
      "loss: 0.036417  [12864/40110]\n",
      "loss: 0.045161  [19264/40110]\n",
      "loss: 0.026660  [25664/40110]\n",
      "loss: 0.115987  [32064/40110]\n",
      "loss: 0.022323  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.059360 \n",
      " Precision: 0.977455, Recall: 0.968238, f1: 0.972825 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.027288  [   64/40110]\n",
      "loss: 0.102340  [ 6464/40110]\n",
      "loss: 0.055532  [12864/40110]\n",
      "loss: 0.096727  [19264/40110]\n",
      "loss: 0.034502  [25664/40110]\n",
      "loss: 0.050488  [32064/40110]\n",
      "loss: 0.043273  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.057915 \n",
      " Precision: 0.968550, Recall: 0.978164, f1: 0.973333 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.022959  [   64/40110]\n",
      "loss: 0.064612  [ 6464/40110]\n",
      "loss: 0.026829  [12864/40110]\n",
      "loss: 0.029825  [19264/40110]\n",
      "loss: 0.162100  [25664/40110]\n",
      "loss: 0.052323  [32064/40110]\n",
      "loss: 0.033711  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.057659 \n",
      " Precision: 0.960723, Recall: 0.989330, f1: 0.974817 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.085011  [   64/40110]\n",
      "loss: 0.057266  [ 6464/40110]\n",
      "loss: 0.146502  [12864/40110]\n",
      "loss: 0.090195  [19264/40110]\n",
      "loss: 0.109160  [25664/40110]\n",
      "loss: 0.036325  [32064/40110]\n",
      "loss: 0.077228  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.051526 \n",
      " Precision: 0.980427, Recall: 0.969479, f1: 0.974922 \n",
      "\n",
      "Saving Checkpoint epoch: 10\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.074433  [   64/40110]\n",
      "loss: 0.095080  [ 6464/40110]\n",
      "loss: 0.133777  [12864/40110]\n",
      "loss: 0.051629  [19264/40110]\n",
      "loss: 0.043247  [25664/40110]\n",
      "loss: 0.107440  [32064/40110]\n",
      "loss: 0.056495  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.056895 \n",
      " Precision: 0.984791, Recall: 0.964020, f1: 0.974295 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.051671  [   64/40110]\n",
      "loss: 0.144589  [ 6464/40110]\n",
      "loss: 0.083918  [12864/40110]\n",
      "loss: 0.093807  [19264/40110]\n",
      "loss: 0.199945  [25664/40110]\n",
      "loss: 0.098478  [32064/40110]\n",
      "loss: 0.050696  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.051776 \n",
      " Precision: 0.979577, Recall: 0.975931, f1: 0.977750 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.025589  [   64/40110]\n",
      "loss: 0.044382  [ 6464/40110]\n",
      "loss: 0.027092  [12864/40110]\n",
      "loss: 0.052438  [19264/40110]\n",
      "loss: 0.042027  [25664/40110]\n",
      "loss: 0.025338  [32064/40110]\n",
      "loss: 0.037742  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.051562 \n",
      " Precision: 0.977612, Recall: 0.975186, f1: 0.976398 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.095984  [   64/40110]\n",
      "loss: 0.027220  [ 6464/40110]\n",
      "loss: 0.104124  [12864/40110]\n",
      "loss: 0.027027  [19264/40110]\n",
      "loss: 0.127637  [25664/40110]\n",
      "loss: 0.023665  [32064/40110]\n",
      "loss: 0.029622  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.050090 \n",
      " Precision: 0.965392, Recall: 0.989826, f1: 0.977457 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.046810  [   64/40110]\n",
      "loss: 0.060709  [ 6464/40110]\n",
      "loss: 0.009508  [12864/40110]\n",
      "loss: 0.009361  [19264/40110]\n",
      "loss: 0.115896  [25664/40110]\n",
      "loss: 0.023060  [32064/40110]\n",
      "loss: 0.050636  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.048176 \n",
      " Precision: 0.969246, Recall: 0.985360, f1: 0.977236 \n",
      "\n",
      "Saving Checkpoint epoch: 15\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.039598  [   64/40110]\n",
      "loss: 0.074944  [ 6464/40110]\n",
      "loss: 0.065739  [12864/40110]\n",
      "loss: 0.018192  [19264/40110]\n",
      "loss: 0.097644  [25664/40110]\n",
      "loss: 0.075653  [32064/40110]\n",
      "loss: 0.117948  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.050944 \n",
      " Precision: 0.975428, Recall: 0.975186, f1: 0.975307 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.043536  [   64/40110]\n",
      "loss: 0.053390  [ 6464/40110]\n",
      "loss: 0.021552  [12864/40110]\n",
      "loss: 0.072314  [19264/40110]\n",
      "loss: 0.018749  [25664/40110]\n",
      "loss: 0.127266  [32064/40110]\n",
      "loss: 0.051167  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.056996 \n",
      " Precision: 0.963169, Recall: 0.986352, f1: 0.974623 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.010933  [   64/40110]\n",
      "loss: 0.022643  [ 6464/40110]\n",
      "loss: 0.082627  [12864/40110]\n",
      "loss: 0.027080  [19264/40110]\n",
      "loss: 0.058482  [25664/40110]\n",
      "loss: 0.040092  [32064/40110]\n",
      "loss: 0.008776  [38464/40110]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046884 \n",
      " Precision: 0.980597, Recall: 0.978164, f1: 0.979379 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.007804  [   64/40110]\n",
      "loss: 0.078635  [ 6464/40110]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "dataset = ImageFolder(train_path, transform=transform)\n",
    "train_data, val_data = random_split(dataset, [0.8, 0.2], generator=generator)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Training {model_name}\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, device, t, writer)\n",
    "    test(val_dataloader, model, loss_fn, device, t, writer)\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        print(f\"Saving Checkpoint epoch: {t}\")\n",
    "        torch.save(model, f\"models/{model_name}-{t}.pt\")\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "Tests the model from the given path on the following metrics:\n",
    "1. Accuracy\n",
    "2. F-1\n",
    "3. Precision\n",
    "4. Recall\n",
    "5. Binary Cross Entropy Loss\n",
    "6. Average run time for sample (either on GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"models\\resnet50-modtrans-130.pt\"\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # needed for alexNet\n",
    "])\n",
    "\n",
    "test_dataset = ImageFolder(test_path, test_transform)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 96.245%, Avg loss: 0.108407 \n",
      " Precision: 0.941662, Recall: 0.969112, f1: 0.955190 Exec Time: 0.013155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference(test_dataloader, model, loss_fn, \"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 97.840%, Avg loss: 0.059477 \n",
      " Precision: 0.974300, Recall: 0.973359, f1: 0.973829 Exec Time: 0.096122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference(test_dataloader, model, loss_fn, \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "Given a path to a trained model, export as a ONNX model so it can be used with the ONNX runtime in JavaScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX runtime in JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'models\\mobilenetv3-l\\mobilenetv3-l-20.pt'\n",
    "\n",
    "model = torch.load().to('cpu')\n",
    "\n",
    "batch_size = 1    # just a random number\n",
    "x = torch.randn(batch_size, 3, 224, 224, requires_grad=True)\n",
    "\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"mobilenetv3-20.onnx\",    # where to save the model (can be a file or file-like object)\n",
    "                  input_names = ['image'],   # the model's input names\n",
    "                  output_names = ['class'],  # the model's output names\n",
    "                  dynamic_axes={'image' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'class' : {0 : 'batch_size'}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
